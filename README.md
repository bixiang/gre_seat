## ML


1.尝试从极大似然估计的角度推导出最小二乘法。
中心极限定理说明大量的随机变量共同作用的结果会倾向于趋近一个正态分布，即记$\epsilon_i=y-x_i^Tw$满足$N(\mu, \sigma^2)$。
而其中的均值可以通过bias调整到零。可得到如下的联合分布
$P(\mu|X, Y)=\prod p(\epsilon_i)=C\times exp(-\frac{\epsilon_i^2}{2 \sigma^2})$
可认为在类似环境下产生的数据方差一样，即认为$\mu_i$独立同分布。
则上式取对数之后，最大化概率值等价于最小化$\sum \epsilon_i^2$，即等价于最小二乘法。

2.当记输入数据为$X \in R^{N\times (p+1)}$，$Y \in R^N$，使用最小二乘法时，试给出相关参数的闭式解。
  $W = (X^TX)^{-1}X^TY$

3。上式存在什么问题，有什么改进方式
$X^TX$不一定可逆
可加入L2正则项，这是最开始使用正则化项的原因。
$W = (X^TX+\lambda I)^{-1}X^TY$，其中 $(X^TX+\lambda I)$是正定矩阵

4.使用L2正则项时，是否包含偏置项，需要对输入进行怎样的处理。
不包含。
需要对数据进行中心化处理。

5.预测得到的 $\hat{y}$、真实值y与题2给出的X的列向量有什么关系？能否从几何的角度解释为什么题2给出的W是最优的。
由$\hat{y}=XW$，若将X进行列分块，则$\hat{y}$为X列向量的线性组合，即其落在X的列空间中。
则为了使得误差项最小，应当使用y在X列空间里的投影作为预测值。
使用最小二乘法时，误差项可以写成如下形式：
$(Xw-y)^T(Xw-y)$，对w求偏导并置零可得$2X^TXW-2X^Ty=0$，整理可得$X^T(XW-y)=X^T\epsilon=0$
则$\epsilon=\hat{y}-y$垂直于$X^T$的任意行向量，即垂直于X的任意列向量，更进一步有$\epsilon$垂直于X的列空间。即此时使用的W满足$\hat{y}$是y在X列空间里的投影的要求。
证毕。


## DL

1在深度学习中，主要采用的是一阶优化还是二阶优化方法，为什么。
主要采用一阶优化方法。
二阶方法的计算量过大，且对于数据有较大的敏感性。

2优化方法中的动量有什么效果，试画图说明
这是一种对于信息求取滑动平均的方式。可以帮助模型更快的到达优值点，减小震荡
图的话就画一个减小震荡快速通过峡谷的就可以了。

3优化中使用的BN算法有什么优点，原因是什么，还有什么类似方法
优点
* 加速收敛
* 可以不用使用L2或是dropout
* 对于学习率的要求降低

原因
* 过去一般认为其可以减少ICS
* 现在也可以从增强L-连续性的角度进行说明

此外还有 IN GN等

4正负样本特别不均衡可以如何处理，cv中有哪些策略是与此相关的。
focal loss
ohem
rpn中的top N策略